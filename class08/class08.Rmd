---
title: "Class 08: Machine Learning"
author: "Frank Lee"
date: "4/26/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Kmeans clustering

Let's try out the **kmeans()** function in R with some made-up data
kmeans(x, center (aka the number of groups), nstart (number of iterations))

```{r}
# Generate some example data for clustering
# Remember that rnorm produces a random normal distribution of 30 samples with a mean of 3 or -3 (a third value would have been the standard deviation)
tmp <- c(rnorm(30,-3), rnorm(30,3))
x <- cbind(x=tmp, y=rev(tmp))
plot(x)
```


Use the kmeans() function setting k to 2 and nstart=20
Inspect/print the results
Q. How many points are in each cluster? 30 and 30
```{r}
km <- kmeans(x, centers = 2, 20)
km
km$size

```


Q. What ‘component’ of your result object details
      - cluster size?  
      - cluster assignment/membership?
      - cluster center?
      
```{r}
# Determines cluster size
km$size

# Determines cluster assignment/membership
km$cluster
# Table should tell us how many 1's and 2's exist
table(km$cluster)

# Detemrines cluster centers
km$centers

```

Plot x colored by the kmeans cluster assignment and
      add cluster centers as blue points
      
```{r}
# We are setting the color based on cluster assignment/membership
plot(x, col=km$cluster)

# This code shows us the points of the centers
points(km$centers, col="blue", pch=18, cex=3)
```

## Hierarchal Clustering

Here we don't have to spell out k (the number of clusters) beforehand but we do have to give it a distance matrix as the input. 

```{r}
d <-dist(x) # distance matrix
hc <- hclust(d)
hc
```

Let's plot the results 

```{r}
plot(hc)
abline(h=6, col="red")
cutree(hc, h=6)
```

What if we made three clusters?
```{r}
gp3 <- cutree(hc, k=3)
gp3
table(gp3)
```

How about two clusters?
```{r}
gp2 <- cutree(hc, k=2)
gp2
table(gp2)
```

30 of the samples in group1 of gp2 are in group1 of gp3
28 of the samples in group2 of gp2 are in group2 of gp3
2 of the samples in group2 of gp2 are in group3 of gp3
```{r}
table(gp2, gp3)
```


Now let's try a more real-life like example to see how our clustering looks. 
```{r}
# Step 1. Generate some example data for clustering
x <- rbind(
  matrix(rnorm(100, mean=0, sd = 0.3), ncol = 2),   # c1
  matrix(rnorm(100, mean = 1, sd = 0.3), ncol = 2), # c2
  matrix(c(rnorm(50, mean = 1, sd = 0.3),           # c3
           rnorm(50, mean = 0, sd = 0.3)), ncol = 2))
colnames(x) <- c("x", "y")
# Step 2. Plot the data without clustering
plot(x)
# Step 3. Generate colors for known clusters
#         (just so we can compare to hclust results)
col <- as.factor( rep(c("c1","c2","c3"), each=50) )
plot(x, col=col)
```

Q. Use the dist(), hclust(), plot() and cutree() functions to return 2 and 3 clusters
```{r}
# For 2 clusters
dist <- dist(x)
hclust <- hclust(dist)
plot2 <- plot(hclust)
abline(h = 1.8, col="red")
cutree2 <- cutree(hclust, k=2)
cutree2
table2 <- table(cutree2)
table2
```

```{r}

#For 3 clusters
cutree3 <- cutree(hclust, k=3)
cutree3
table3 <- table(cutree3)
table3
```

```{r}
table(cutree2, cutree3)
```


Q. How does this compare to your known 'col' groups?

```{r}
plot(x, col=cutree3)
```



## Principal Component Analysis (PCA)

Let's get some RNASeq data to play with

```{r}
mydata <- read.csv("https://tinyurl.com/expression-CSV",
                  row.names=1)
mydata
head(mydata)

# How many genes are in this dataset?
ngenes <- nrow(mydata)
```

There are 'r nrow(mydata)' genes in this dataset. We can include R code in our text by typing 'r (insert function here)'.

Let's do PCA
NOTE: prcomp() expects the samples to be rows and genes to be columns so we need to first transpose the matrix with the t() function!

```{r}
# t = transpose
# Now your genes (which were originally rows) are now columns
t(mydata)

pca <- prcomp(t(mydata), scale = TRUE)
pca
summary(pca)
```


```{r}
attributes(pca)

# x is the new plotted points from the PCA
```

Let's make our first PCA plot
```{r}
plot(pca$x[,1], pca$x[,2])
```


```{r}
## lets do PCA
 pca <- prcomp(t(mydata), scale=TRUE)
 ## A basic PC1 vs PC2 2-D plot
 plot(pca$x[,1], pca$x[,2])
## Percent variance is often more informative to look at
pca.var <- pca$sdev^2
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)
pca.var.per
```

```{r}
xlab <- paste("PC1(",pca.var.per[1],"%)", sep="")
ylab <- paste("PC2(",pca.var.per[2],"%)", sep="")

xlab
ylab

mycols <- c(rep("red", 5), rep("blue", 5))
```


```{r}
plot(pca$x[,1], pca$x[,2], xlab=xlab, ylab=ylab, col = mycols)
text(pca$x[,1], pca$x[,2], colnames(mydata))
```


You can use idnetify(pca$x[,1], pca$x[,2], colnames(mydata)) in the console to click on a point in the printed plot to label them individually. 


## Now let's do PCA on part 2 of the UK foods


```{r}
x <- read.csv("UK_foods.csv")
x

## Complete the following code to find out how many rows and columns are in x?
dim <- dim(x)
numcol <- ncol(x)
numrow <- nrow(x)

```


```{r}
## Preview the first 6 rows
head(x)
```


Hmm, it looks like the row-names here were not set properly as we were expecting 4 columns (one for each of the 4 countries of the UK - not 5 as reported from the dim() function).

Here it appears that the row-names are incorrectly set as the first column of our x data frame (rather than set as proper row-names). This is very common error. Lets try to fix this up with the following code, which sets the rownames() to the first column and then removes the troublesome first column (with the -1 column index):

```{r}
# Note how the minus indexing works
rownames(x) <- x[,1]

# The minus means that you include everything except the first column
x <- x[,-1]
# or, you could do this
#x <- x[,-c(1,2)] To get rid of the first 2 columns
head(x)
```

This looks much better, now lets check the dimensions again:

```{r}
dim(x)

# or, we could have done:
x <- read.csv("UK_foods.csv", row.names=1)
head(x)

# Actually, this method is better!!!
```


Q2. Which approach to solving the ‘row-names problem’ mentioned above do you prefer and why? Is one approach more robust than another under certain circumstances?

A: I prefer the second method because it's simpler. And if you run the first method multiple times, the columns disappear one by one. 



A cursory glance over the numbers in this table does not reveal much of anything. Indeed in general it is difficult to extract meaning in regard to major differences and trends from any given array of numbers. Generating regular bar-plots and various pairwise plots does not help too much either:

```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))

```

Q3: Changing what optional argument in the above barplot() function results in the following plot?

The beside argument! Make it false to prevent the data from being presented beside each other. 

```{r}
barplot(as.matrix(x), beside = F, col=rainbow(nrow(x)))
```


Q5: Generating all pairwise plots may help somewhat. Can you make sense of the following code and resulting figure? What does it mean if a given point lies on the diagonal for a given plot?

A: Look at a plot, then see the col/row of the two countries it relates to. The country in the vertical line is the y axis, the country in the horizontal line is the x axis. If the dot is off the diagonal, then one of the countries has more of the thing than the other. 

```{r}
pairs(x, col=rainbow(10), pch=16)
```

Even relatively small datasets can prove chalanging to interpertate Given that it is quite difficult to make sense of even this relatively small data set. Hopefully, we can clearly see that a powerful analytical method is absolutely necessary if we wish to observe trends and patterns in larger datasets.


Q6. What is the main differences between N. Ireland and the other countries of the UK in terms of this data-set?

A: North Ireland has a lower blue dot than the other countries. 




We need to make sense of this data. PCA to the rescue!

```{r}
# Use the prcomp() PCA function 
pca <- prcomp( t(x) )
summary(pca)
```

```{r}
# Plot PC1 vs PC2
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2", xlim=c(-270,500))
text(pca$x[,1], pca$x[,2], colnames(x))
```

Q8. Customize your plot so that the colors of the country names match the colors in our UK and Ireland map and table at start of this document.

```{r}
mycols <- c("orange", "red", "blue", "darkgreen")

plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2", xlim=c(-270,500))
text(pca$x[,1], pca$x[,2], colnames(x), col=mycols)
abline(h=0, col="gray", lty=2)
abline(v=0, col="gray", lty=2)

```

Below we can use the square of pca$sdev , which stands for “standard deviation”, to calculate how much variation in the original data each PC accounts for.

```{r}
v <- round( pca$sdev^2/sum(pca$sdev^2) * 100 )
v
```


```{r}
## or the second row here...
z <- summary(pca)
z$importance
```

This information can be summarized in a plot of the variances (eigenvalues) with respect to the principal component number (eigenvector number), which is given below.
```{r}
barplot(v, xlab="Principal Component", ylab="Percent Variation")
```

Digging deeper....
We can also consider the influence of each of the original variables upon the principal components (typically known as *loading scores*). This information can be obtained from the prcomp() returned $rotation component. It can also be summarized with a call to biplot(), see below:

```{r}
## Lets focus on PC1 as it accounts for > 90% of variance 
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,1], las=2 )
```
Here we see observations (foods) with the largest positive loading scores that effectively “push” N. Ireland to *right positive side* of the plot (including Fresh_potatoes and Soft_drinks).

We can also see the observations/foods with high negative scores that push the other countries to the *left side* of the plot (including Fresh_fruit and Alcoholic_drinks).


Q9: Generate a similar ‘loadings plot’ for PC2. What two food groups feature prominantely and what does PC2 maninly tell us about?

A: 



Another way to see this information together with the main PCA plot is in a so-called biplot:
```{r}
## The inbuilt biplot() can be useful for small datasets 
biplot(pca)
```


## PCA of RNA-Seq data


```{r}
rna.data <- read.csv("expression.csv", row.names=1)
head(rna.data)

ngenes <- nrow(rna.data)
nsamples <- ncol(rna.data)
```



