---
title: "Class 09 Unsupervised Learning"
author: "Frank Lee"
date: "5/1/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Q1: What type of object is returned from the read.csv function? Check the right in the wisc.df data. It's a data frame.
```{r}

fna.data <- "data/WisconsinCancer.csv"
wisc.df <- read.csv("WisconsinCancer.csv")
wisc.df
```

Q2: How many patients are there? Use nrow to count the number of rows. 
```{r}
numPatients <- nrow(wisc.df)
numCol <- ncol(wisc.df)
```


Q3: How many observations have a malignant diagnosis?
```{r}
table(wisc.df$diagnosis)
```

Q4: How many variables/features in the data are suffixed with _mean?
```{r}
grep("_mean", colnames(wisc.df))
length(grep("_mean", colnames(wisc.df)))
```

Q5: Why do you think we're using the indices 3:32 here?
The first 2 columns and the last column are not numerical values, but string values. 
```{r}
wisc.df
View(wisc.df)
wisc.data <- as.matrix(wisc.df[,3:32])
wisc.data
```


```{r}
row.names(wisc.data) <- wisc.df$id
wisc.data

diagnosis <- wisc.df$diagnosis
diagnosis
```

Now the next step is to perform principal component analysis (PCA) on wisc.data.

Do I need to scale the data before PCA? Check the col means and standard deviations. 
```{r}
# Find the means
# Use round to avoid scientific notation. The second input is the number of sig figs you want. 
round(colMeans(wisc.data), 1)

# 1 means you apply the function across the rows
# 2 means you apply the function across the columns

# Now find the standard deviations
round(apply(wisc.data,2,sd), 1)

# You could also find the means like this
round(apply(wisc.data,2, mean), 1)
```

Based on these results, we need to scale the data because the means and sd's are quite different. 

Generally (as in this case) variables are scaled to have a standard deviation of one and a mean of zero. Such scaling is particular recommended when variables are measured using different scales or when the original mean and/or standard deviation of variables is largely different. 

Now, perform PCA on wisc.data by completing the following code.
```{r}
wisc.pr <- prcomp(wisc.data, scale = TRUE)
summary(wisc.pr)
```

Q7. From your results, what proportion of the original variance is captured by the first principal components (PC1)? 0.4427

Q8. How many principal components (PCs) are required to describe at least 70% of the original variance in the data? 3

Q9. How many principal components (PCs) are required to describe at least 90% of the original variance in the data? 


Now you will use some visualizations to better understand your PCA model. A common visualization for PCA results is the so-called biplot.

Q10. What stands out to you about this plot? Is it easy or difficult to understand? Why?
```{r}
biplot(wisc.pr)
```
It is quite crap really! This default plot is only helpful for relatively small datasets in my experience.


Q11. Generate a PC1 vs PC2 plot as described above colored by diagnosis. What do the points in this plot represent? What are the red points? Where did this coloring come from and why were these colors chosen by R?
```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col = diagnosis, xlab = "PC1", ylab = "PC2")
```


Q12. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?
```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,3], col = diagnosis, 
     xlab = "PC1", ylab = "PC3")
```

```{r}
plot(wisc.pr$x[,2], wisc.pr$x[,3], col = diagnosis, 
     xlab = "PC2", ylab = "PC3")
```

PC3 only captures 9% of the variance (based on the proportion of variance). PC1 vs. PC2 demonstrates the best plot. 


Calculate the variance of each component.
```{r}
pr.var <- wisc.pr$sdev^2
head(pr.var)
```

Now look at the variance explained by each principal component divided by the total variance explained of all principal components. 
```{r}
pve <-  pr.var/sum(pr.var)

plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```

Alternative scree plot of the same data, note data driven y-axis
```{r}
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```

In this section we will check your understanding of the PCA results, in particular the loadings and variance explained. The loadings, represented as vectors, explain the mapping from the original features to the principal components. The principal components are naturally ordered from the most variance explained to the least variance explained.


Q13. For the first principal component, and using two significant figures , what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature  radius_mean?
```{r}
round(wisc.pr$rotation[,1], 2)

# For the feature radius_mean:
# Access the contribution of one variable on PC1
round(wisc.pr$rotation["radius_mean",1], 2)

```

Q14. For the first principal component, and using two significant figures, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature  smoothness_se?
```{r}
# Access the contribution of one variable on PC1
round(wisc.pr$rotation["smoothness_se",1], 2)

```

Q15. Which original variable contributes most to PC1?
```{r}
# Sort the loadings values by their absolute contribution to PC1
order <- sort(abs(wisc.pr$rotation[,1]), decreasing = TRUE)
order[1]

```


## Hierarchal Clustering

Scale the wisc.data and assign the result to data.scaled
```{r}
data.scaled <- scale(wisc.data)
data.scaled
```

Calculate the (Euclidean) distances between all pairs of observations in the new scaled dataset and assign the result to data.dist.
```{r}
data.dist <-dist(data.scaled)
head(data.dist)
```

Create a hierarchical clustering model using complete linkage. Manually specify the method argument to hclust() and assign the results to wisc.hclust.

```{r}
wisc.hclust <- hclust(data.dist, method = "complete")
head(wisc.hclust)
```


Q16. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?

```{r}
plot(wisc.hclust)
abline(h=19, col="red", lty=2)
```


```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k=4)

# This does the same thing as the code above, since four clusters form at height 19
wisc.hclust.clusters <- cutree(wisc.hclust, h=19)
table(wisc.hclust.clusters)
table(wisc.hclust.clusters, diagnosis)
```

Q17. Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10? How would you determine what better is in this context?

```{r}
wisc.hclust.clusters <-  cutree(wisc.hclust, 7)
table(wisc.hclust.clusters, diagnosis)

```



## Skipping to section 5: Combining methods (PCA + hclust)

In this final section, you will put together several steps you used earlier and, in doing so, you will experience some of the creativity and open endedness that is typical in unsupervised learning.

Using the minimum number of principal components required to describe at least 90% of the variability in the data, create a hierarchical clustering model with the linkage method="ward.D2". We use Ward's criterion here because it is based on multidimensional variance like principal components analysis. Assign the results to wisc.pr.hclust.

I am going to start with the PCs that capture 90% of the original variance in the dataset (i.e. PC1 to PC7).

```{r}
wisc.pr$x[,1:7]
wisc.pr.hclust <- hclust(dist(wisc.pr$x[,1:7]), method = "ward.D2")
plot(wisc.pr.hclust)
```

```{r}
grps <- cutree(wisc.pr.hclust, k=2)
table(grps)
```

```{r}
table(grps, diagnosis)

```

```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col=grps)

# This does the same thing
# plot(wisc.pr$x[,1:2], col=grps)
```

```{r}
plot(wisc.pr$x[,1:2], col=diagnosis)

```

Note the color swap here as the hclust cluster 1 is mostly "M" and cluster 2 is mostly "B" as we saw from the results of calling table(grps, diagnosis). To match things up we can turn our groups into a factor and reorder the levels so cluster 2 comes first

```{r}
g <- as.factor(grps)
levels(g)
```

```{r}
g <- relevel(g,2)
levels(g)
```


```{r}
plot(wisc.pr$x[,1:2], col=g)

```












## Attempt at Section 4

```{r}
library(rgl)
plot3d(wisc.pr$x[,1:3], xlab="PC 1", ylab="PC 2", zlab="PC 3", cex=1.5, size=1, type="s", col=grps)
```

```{r}
wisc.km <- kmeans(wisc.pr$x, centers=2, nstart= 20)
table(wisc.km$cluster, diagnosis)
```

```{r}
table(wisc.hclust.clusters, wisc.km$cluster)
```









